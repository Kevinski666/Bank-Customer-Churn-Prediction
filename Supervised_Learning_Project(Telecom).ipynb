{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised Learning Project(Telecom).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R88Ms0MTi0Ma"
      },
      "source": [
        "# User Churn Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WA6lL1fni0Mb"
      },
      "source": [
        "**In** this project, we use supervised learning models to identify customers who are likely to stop using service in the future. Furthermore, we will analyze top factors that influence user retention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bO94-bXZi0Md"
      },
      "source": [
        "## Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SIvRSRqAi0Md"
      },
      "source": [
        "<ul>\n",
        "<li>[Part 1: Data Exploration](#Part-1:-Data-Exploration)\n",
        "<li>[Part 2: Feature Preprocessing](#Part-2:-Feature-Preprocessing)\n",
        "<li>[Part 3: Model Training and Results Evaluation](#Part-3:-Model-Training-and-Result-Evaluation)\n",
        "<li>[Part 4: Feature Selection](#Part-4:-Feature-Selection)\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TUoI2S7Bi6iR"
      },
      "source": [
        "# Part 0: Setup Google Drive Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "neechzbWi7rV",
        "colab": {}
      },
      "source": [
        "# method 1 install pydrive to load data\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UScKyL2TjARW",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1JczT5KaTncUy0GabzoRAEfcvjFPQSYF2'\n",
        "fluff, id = link.split('=')\n",
        "file = drive.CreateFile({'id':id}) # replace the id with id of file you want to access\n",
        "file.GetContentFile('churn.all')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nK7A1qhYSDxM",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('churn.all')\n",
        "df1.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhMVchpMPjRc",
        "colab": {}
      },
      "source": [
        "# method 2 upload from local\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ikYHWOnYfKPM"
      },
      "source": [
        "**Homework 0: Use the recommended way to load data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "1iLCfrlpe-pV",
        "colab": {}
      },
      "source": [
        "#@title Homework 0\n",
        "####\n",
        "## Your Code\n",
        "####\n",
        "import pandas as pd\n",
        "file_id='1BmXe8XIPvbghz7kMEjmt9vBX8LZbX4Jq'\n",
        "link='https://drive.google.com/uc?export=download&id={FILE_ID}'\n",
        "csv_url=link.format(FILE_ID=file_id)\n",
        "my_data = pd.read_csv(csv_url)\n",
        "my_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a6bG_gAPi0Me"
      },
      "source": [
        "# Part 1: Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bspx2K6fi0Me"
      },
      "source": [
        "### Part 1.1: Understand the Raw Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kuTHKjk-i0Mf",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "churn_df = pd.read_csv('churn.all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "CtUqK6j1fhDN",
        "colab": {}
      },
      "source": [
        "#@title *Bonus (不需要掌握): The Iris dataset in sklearn*\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "print(type(iris)) # bunch like dictionary\n",
        "print(iris.keys())\n",
        "\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hHNZRs2Ti0Mi",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "churn_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C99Z9b7ai0Mm",
        "colab": {}
      },
      "source": [
        "print (\"Num of rows: \" + str(churn_df.shape[0])) # row count\n",
        "print (\"Num of columns: \" + str(churn_df.shape[1])) # col count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OCglmJ9Oi0Mo"
      },
      "source": [
        "### Part 1.2: Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JxlrXRG3i0Mp"
      },
      "source": [
        "Remove Extra Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vf8iYmWi0Mq",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# check categorical feature\n",
        "churn_df['voice_mail_plan'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3lpwxvQfi0Mt",
        "colab": {}
      },
      "source": [
        "# remove the heading and trailing whitespaces\n",
        "churn_df['voice_mail_plan'] = churn_df['voice_mail_plan'].apply(lambda x: x.strip())\n",
        "churn_df['intl_plan'] = churn_df['intl_plan'].apply(lambda x: x.strip())\n",
        "churn_df['churned'] = churn_df['churned'].apply(lambda x: x.strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kcyHhHKHZN2p",
        "colab": {}
      },
      "source": [
        "# check the categorical feature after manipulation\n",
        "churn_df['voice_mail_plan'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SsAbAjhvi0Mx"
      },
      "source": [
        "### Part 1.3:  Understand the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rJ0AdxwLi0Mz",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# check the feature distribution\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.distplot(churn_df['total_intl_charge'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4DKTTdB6i0M2",
        "colab": {}
      },
      "source": [
        "# correlations between all the features\n",
        "corr = churn_df[[\"account_length\", \"number_vmail_messages\", \"total_day_minutes\",\n",
        "                    \"total_day_calls\", \"total_day_charge\", \"total_eve_minutes\",\n",
        "                    \"total_eve_calls\", \"total_eve_charge\", \"total_night_minutes\",\n",
        "                    \"total_night_calls\", \"total_intl_minutes\", \"total_intl_calls\",\n",
        "                    \"total_intl_charge\"]].corr()\n",
        "\n",
        "# show heapmap of correlations\n",
        "sns.heatmap(corr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1qfEnNW_i0M5",
        "colab": {}
      },
      "source": [
        "# check the actual values of correlations\n",
        "corr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aFa4d6t3i0NH"
      },
      "source": [
        "# Part 2: Feature Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wtjI61m6i0M8",
        "colab": {}
      },
      "source": [
        "# calculate two features correlation\n",
        "from scipy.stats import pearsonr\n",
        "print (pearsonr(churn_df['total_day_minutes'], churn_df['number_vmail_messages'])[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pxtf6XoJi0NI",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4ec5r_Qdi0NL",
        "colab": {}
      },
      "source": [
        "# Get ground truth data\n",
        "y = np.where(churn_df['churned'] == 'True.',1,0)\n",
        "\n",
        "# Drop some useless columns\n",
        "to_drop = ['state','area_code','phone_number','churned']\n",
        "churn_feat_space = churn_df.drop(to_drop, axis=1)\n",
        "\n",
        "# yes and no have to be converted to boolean values\n",
        "yes_no_cols = [\"intl_plan\",\"voice_mail_plan\"]\n",
        "churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'\n",
        "\n",
        "X = churn_feat_space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS0VMSSpCLq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "churn_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rzCo_GC97rGd",
        "colab": {}
      },
      "source": [
        "# check the propotion of y = 1\n",
        "print(y.sum() / y.shape * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oNsZwmWxi0NP"
      },
      "source": [
        "#### Homework 1: Can you add catogorical features, e.g. state, into your feature matrix?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JMTIEpY7IfPp"
      },
      "source": [
        "Read more for handling [categorical feature](https://github.com/scikit-learn-contrib/categorical-encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "MrsjiqTwi0NQ",
        "colab": {}
      },
      "source": [
        "#@title Homework 1\n",
        "####\n",
        "## Your Code\n",
        "####\n",
        "\n",
        "to_drop_hw1 = ['area_code','phone_number','churned']\n",
        "churn_feat_space_hw1 = churn_df.drop(to_drop_hw1, axis=1)\n",
        "\n",
        "# yes and no have to be converted to boolean values\n",
        "yes_no_cols = [\"intl_plan\",\"voice_mail_plan\"]\n",
        "churn_feat_space_hw1[yes_no_cols] = churn_feat_space_hw1[yes_no_cols] == 'yes'\n",
        "\n",
        "# sklearn.preprocessing.OneHotEncoder\n",
        "churn_feat_space_hw1 = pd.get_dummies(churn_feat_space_hw1, columns=['state'])\n",
        "\n",
        "churn_feat_space_hw1.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q3x9ySX_i0Nd"
      },
      "source": [
        "# Part 3: Model Training and Result Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "77OjmSl9i0Nf"
      },
      "source": [
        "### Part 3.1: Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uay8Md5li0Nh",
        "colab": {}
      },
      "source": [
        "# Splite data into training and testing\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Reserve 20% for testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print('training data has %d observation with %d features'% X_train.shape)\n",
        "print('test data has %d observation with %d features'% X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JuPhtUkJi0NW",
        "colab": {}
      },
      "source": [
        "# Scale the data, using standardization\n",
        "# standardization (x-mean)/std\n",
        "# normalization (x-x_min)/(x_max-x_min)\n",
        "# 1. speed up gradient descent\n",
        "# 2. same scale\n",
        "\n",
        "# for example, use training data to train the standardscaler to get mean and std \n",
        "# apply mean and std to both training and testing data.\n",
        "# fit_transform does the training and applying, transform only does applying.\n",
        "# Because we can't use any info from test, and we need to do the same modification\n",
        "# to testing data as well as training data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c4UTtCQTi0Nl"
      },
      "source": [
        "### Part 3.2: Model Training and Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "EAhSxINLi0Nl",
        "colab": {}
      },
      "source": [
        "#@title build models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Logistic Regression\n",
        "classifier_logistic = LogisticRegression()\n",
        "\n",
        "# K Nearest Neighbors\n",
        "classifier_KNN = KNeighborsClassifier()\n",
        "\n",
        "# Random Forest\n",
        "classifier_RF = RandomForestClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Av0IRSoBQ3pe",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "classifier_logistic.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EiLuzUDJRBNi",
        "colab": {}
      },
      "source": [
        "# Prediction of test data\n",
        "classifier_logistic.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XjMV04mKRJ30",
        "colab": {}
      },
      "source": [
        "# Accuracy of test data\n",
        "classifier_logistic.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1OCgNSNri0Nn",
        "colab": {}
      },
      "source": [
        "# Use 5-fold Cross Validation to get the accuracy for different models\n",
        "model_names = ['Logistic Regression','KNN','Random Forest']\n",
        "model_list = [classifier_logistic, classifier_KNN, classifier_RF]\n",
        "count = 0\n",
        "\n",
        "for classifier in model_list:\n",
        "    cv_score = model_selection.cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "    print(cv_score)\n",
        "    print('Model accuracy of %s is: %.3f'%(model_names[count],cv_score.mean()))\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G1GCzaPxi0Np"
      },
      "source": [
        "#### Homework 2: Can you do prediction with SVM model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "yxmKTdahi0Nq",
        "colab": {}
      },
      "source": [
        "#@title Homework 2\n",
        "####\n",
        "## Your Code\n",
        "####\n",
        "# SVC\n",
        "from sklearn.svm import SVC \n",
        "\n",
        "classifier_SVC = SVC()\n",
        "\n",
        "cv_score = model_selection.cross_val_score(classifier_SVC, X_train, y_train, cv=5)\n",
        "print('Model accuracy of SVM is: %.3f'%(cv_score.mean()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7J-23z78i0Ns"
      },
      "source": [
        "### (Optional) Part 3.3: Use Grid Search to Find Optimal Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hpe9PEAAi0Nt",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# helper function for printing out grid search results \n",
        "def print_grid_search_metrics(gs):\n",
        "    print (\"Best score: %0.3f\" % gs.best_score_)\n",
        "    print (\"Best parameters set:\")\n",
        "    best_parameters = gs.best_params_\n",
        "    for param_name in sorted(parameters.keys()):\n",
        "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qvYo9I5Ti0Nv"
      },
      "source": [
        "#### Part 3.3.1: Find Optimal Hyperparameters - LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wOc48syxi0Nx",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for Logistic Regression Regularization\n",
        "# Penalty is choosed from L1 or L2\n",
        "# C is the lambda value(weight) for L1 and L2\n",
        "\n",
        "# ('l1', 1) ('l1', 5), ('l1', 10) ('l2', 1) ('l2', 5), ('l2', 10)\n",
        "parameters = {\n",
        "    'penalty':('l1', 'l2'), \n",
        "    'C':(1, 5, 10)\n",
        "}\n",
        "Grid_LR = GridSearchCV(LogisticRegression(),parameters, cv=5)\n",
        "Grid_LR.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nN5rU0e-i0N1",
        "colab": {}
      },
      "source": [
        "# the best hyperparameter combination\n",
        "print_grid_search_metrics(Grid_LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TtkDsXgui0N3",
        "colab": {}
      },
      "source": [
        "# best model\n",
        "best_LR_model = Grid_LR.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9u9YFedOi0N6"
      },
      "source": [
        "#### Part 3.3.2: Find Optimal Hyperparameters: KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o78422XVi0N6",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for KNN\n",
        "# Choose k\n",
        "parameters = {\n",
        "    'n_neighbors':[3,5,7,10] \n",
        "}\n",
        "Grid_KNN = GridSearchCV(KNeighborsClassifier(),parameters, cv=5)\n",
        "Grid_KNN.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ydaRZVAIi0N_",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# best k\n",
        "print_grid_search_metrics(Grid_KNN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nKn_oKLSi0OB"
      },
      "source": [
        "#### Part 3.3.3: Find Optimal Hyperparameters: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NniAZIPfi0OC",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for Random Forest\n",
        "# Choose the number of trees\n",
        "parameters = {\n",
        "    'n_estimators' : [40,60,80]\n",
        "}\n",
        "Grid_RF = GridSearchCV(RandomForestClassifier(),parameters, cv=5)\n",
        "Grid_RF.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ScPiI-Bfi0OE",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# best number of tress\n",
        "print_grid_search_metrics(Grid_RF)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xJgfri_Mi0OG",
        "colab": {}
      },
      "source": [
        "# best random forest\n",
        "best_RF_model = Grid_RF.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xxDAOrGIi0OI"
      },
      "source": [
        "### Part 3.4: Model Evaluation - Confusion Matrix (Precision, Recall, Accuracy)\n",
        "\n",
        "class of interest as positive\n",
        "\n",
        "TP: correctly labeled real churn\n",
        "\n",
        "Precision(PPV, positive predictive value): tp / (tp + fp);\n",
        "Total number of true predictive churn divided by the total number of predictive churn;\n",
        "High Precision means low fp, not many return users were predicted as churn users. \n",
        "\n",
        "\n",
        "Recall(sensitivity, hit rate, true positive rate): tp / (tp + fn)\n",
        "Predict most postive or churn user correctly. High recall means low fn, not many churn users were predicted as return users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-tP94iFi0OI",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# calculate accuracy, precision and recall, [[tn, fp],[]]\n",
        "def cal_evaluation(classifier, cm):\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    tp = cm[1][1]\n",
        "    accuracy  = (tp + tn) / (tp + fp + fn + tn + 0.0)\n",
        "    precision = tp / (tp + fp + 0.0)\n",
        "    recall = tp / (tp + fn + 0.0)\n",
        "    print (classifier)\n",
        "    print (\"Accuracy is: %0.3f\" % accuracy)\n",
        "    print (\"precision is: %0.3f\" % precision)\n",
        "    print (\"recall is: %0.3f\" % recall)\n",
        "\n",
        "# print out confusion matrices\n",
        "def draw_confusion_matrices(confusion_matricies):\n",
        "    class_names = ['Not','Churn']\n",
        "    for cm in confusion_matrices:\n",
        "        classifier, cm = cm[0], cm[1]\n",
        "        cal_evaluation(classifier, cm)\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap('Reds'))\n",
        "        plt.title('Confusion matrix for %s' % classifier)\n",
        "        fig.colorbar(cax)\n",
        "        ax.set_xticklabels([''] + class_names)\n",
        "        ax.set_yticklabels([''] + class_names)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OpSGaN49i0OL",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Confusion matrix, accuracy, precison and recall for random forest and logistic regression\n",
        "confusion_matrices = [\n",
        "    (\"Random Forest\", confusion_matrix(y_test,best_RF_model.predict(X_test))),\n",
        "    (\"Logistic Regression\", confusion_matrix(y_test,best_LR_model.predict(X_test))),\n",
        "]\n",
        "\n",
        "draw_confusion_matrices(confusion_matrices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OvHlyhPBi0OT"
      },
      "source": [
        "### Part 3.4: Model Evaluation - ROC & AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jx_3XkgKi0OW"
      },
      "source": [
        "RandomForestClassifier, KNeighborsClassifier and LogisticRegression have predict_prob() function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Os_ZLTvi0OX"
      },
      "source": [
        "#### Part 3.4.1: ROC of RF Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UypvQMVBi0OY",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn import metrics\n",
        "\n",
        "# Use predict_proba to get the probability results of Random Forest\n",
        "y_pred_rf = best_RF_model.predict_proba(X_test)[:, 1]\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s3PR-PdPi0Ob",
        "colab": {}
      },
      "source": [
        "# ROC curve of Random Forest result\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve - RF model')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R89IUMYDi0Oe",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# AUC score\n",
        "metrics.auc(fpr_rf,tpr_rf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-1DVqnJVi0Oh"
      },
      "source": [
        "#### Part 3.4.1: ROC of LR Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t-q5XJPoi0Oi",
        "colab": {}
      },
      "source": [
        "# Use predict_proba to get the probability results of Logistic Regression\n",
        "y_pred_lr = best_LR_model.predict_proba(X_test)[:, 1]\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KZSrN-1Mi0Ok",
        "colab": {}
      },
      "source": [
        "# ROC Curve\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_lr, tpr_lr, label='LR')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve - LR Model')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LHAyxishi0On",
        "colab": {}
      },
      "source": [
        "# AUC score\n",
        "metrics.auc(fpr_lr,tpr_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gHHurD8Ii0Oq"
      },
      "source": [
        "# Part 4: Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dSx4TPO-i0Or"
      },
      "source": [
        "### Part 4.1:  Logistic Regression Model - Feature Selection Discussion "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BtLHUixoi0Ot"
      },
      "source": [
        "The corelated features that we are interested in: (total_day_minutes, total_day_charge), (total_eve_minutes, total_eve_charge), (total_intl_minutes, total_intl_charge)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cQaXOIsUi0Ou",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# add L1 regularization to logistic regression\n",
        "# check the coef for feature selection\n",
        "scaler = StandardScaler()\n",
        "X_l1 = scaler.fit_transform(X)\n",
        "LRmodel_l1 = LogisticRegression(penalty=\"l1\", C = 0.1, solver='liblinear')\n",
        "LRmodel_l1.fit(X_l1, y)\n",
        "LRmodel_l1.coef_[0]\n",
        "print (\"Logistic Regression (L1) Coefficients\")\n",
        "for k,v in sorted(zip(map(lambda x: round(x, 4), LRmodel_l1.coef_[0]), \\\n",
        "                      churn_feat_space.columns), key=lambda k_v:(-abs(k_v[0]),k_v[1])):\n",
        "    print (v + \": \" + str(k))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "majifZZqi0O9",
        "colab": {}
      },
      "source": [
        "# add L2 regularization to logistic regression\n",
        "# check the coef for feature selection\n",
        "scaler = StandardScaler()\n",
        "X_l2 = scaler.fit_transform(X)\n",
        "LRmodel_l2 = LogisticRegression(penalty=\"l2\", C = 0.1)\n",
        "LRmodel_l2.fit(X_l2, y)\n",
        "LRmodel_l2.coef_[0]\n",
        "print (\"Logistic Regression (L2) Coefficients\")\n",
        "for k,v in sorted(zip(map(lambda x: round(x, 4), LRmodel_l2.coef_[0]), \\\n",
        "                      churn_feat_space.columns), key=lambda k_v:(-abs(k_v[0]),k_v[1])):\n",
        "    print (v + \": \" + str(k))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uqs41ydLi0O_"
      },
      "source": [
        "### Part 4.2:  Random Forest Model - Feature Importance Discussion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MPxUM2lei0PA",
        "colab": {}
      },
      "source": [
        "# check feature importance of random forest for feature selection\n",
        "forest = RandomForestClassifier()\n",
        "forest.fit(X, y)\n",
        "\n",
        "importances = forest.feature_importances_\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature importance ranking by Random Forest Model:\")\n",
        "for k,v in sorted(zip(map(lambda x: round(x, 4), importances), churn_feat_space.columns), reverse=True):\n",
        "    print (v + \": \" + str(k))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}